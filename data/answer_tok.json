{"class_name": "Tokenizer", "config": {"num_words": 10000, "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n", "lower": false, "split": " ", "char_level": false, "oov_token": "UNK", "document_count": 8, "word_counts": "{\"STARTTAG\": 8, \"Portland\": 8, \"Trail\": 8, \"Blazers\": 8, \"ENDTAG\": 8}", "word_docs": "{\"Trail\": 8, \"Portland\": 8, \"STARTTAG\": 8, \"Blazers\": 8, \"ENDTAG\": 8}", "index_docs": "{\"4\": 8, \"3\": 8, \"2\": 8, \"5\": 8, \"6\": 8}", "index_word": "{\"1\": \"UNK\", \"2\": \"STARTTAG\", \"3\": \"Portland\", \"4\": \"Trail\", \"5\": \"Blazers\", \"6\": \"ENDTAG\"}", "word_index": "{\"UNK\": 1, \"STARTTAG\": 2, \"Portland\": 3, \"Trail\": 4, \"Blazers\": 5, \"ENDTAG\": 6}"}}